nycflights %>% group_by(dep_type) %>% summarise(ot_arr_rate = sum(arr_type == "on time") / n()) %>%
arrange(desc(ot_arr_rate))
nycflights %>% group_by(month) %>% summarise(mean_dd = mean(dep_delay)) %>%
arrange(desc(mean_dd))
ggplot(nycflights, aes(x = factor(month), y = dep_delay)) + geom_boxplot()
ggplot(data = nycflights, aes(x = dep_delay)) + geom_histogram()
nycflights <- nycflights %>% mutate(dep_type = ifelse(dep_delay < 5, "on time", "delayed"))
nycflights %>% group_by(origin) %>% summarise(ot_dep_rate = sum(dep_type == "on time") / n()) %>%
arrange(desc(ot_dep_rate))
ggplot(data = nycflights, aes(x = origin, fill = dep_type)) + geom_bar()
nycflights <- nycflights %>% mutate(avg_speed = (distance / air_time/60))
nycflights %>% arrange(desc(avg_speed)) %>% select(avg_speed,tailnum)
ggplot(nycflights, aes(x = distance, y = avg_speed)) + geom_point()
nycflights <- nycflights %>% mutate(arr_type = ifelse(arr_delay <= 0, "on time", "delayed"))
nycflights %>% group_by(dep_type) %>% summarise(ot_arr_rate = sum(arr_type == "on time") / n()) %>%
arrange(desc(ot_arr_rate))
- You\'re only recommending similar things to what the user has already liked, so the recommendations are often not surprising
rm(list = ls(all = TRUE))
#*********UNIT 6 - CLUSTERING*************
'Netflix = online DVD rental + streaming video service w/ 40M+ subscribers worldwide + annual revenue of $3.6B
A key aspect of the company is being able to offer customers accurate movie recommendations based on a their own preferences
+ viewing history.
From 2006-2009, Netflix ran a contest asking the public to submit algorithms to predict user ratings for movies which would
be useful for Netflix when making recommendations to users.
Netflix provided a training data set of about 100M user ratings + a test data set of about 3M user ratings + offered a grand
prize of $1M to the team who could beat Netflix\'s current algorithm, called Cinematch, by more than 10%, measured in terms
of RMSE
Netflix believed that their recommendation system was so valuable that it was worth $1M to improve.
The contest had a few rules + 1 was that if the grand prize was not yet reached, progress prizes of $50k per yr would be
awarded for the best result so far, as long as it was at least a 1% improvement over the previous year.
Another rule was that teams must submit their code + a description of the algorithm to be awarded any prizes.
Lastly, if a team met the 10% improvement goal, a last call would be issued, + 30 days would remain for all teams to submit
their best algorithm.
The contest went live on October 2, 2006 + by October 8, only 6 days later, a team submitted an algorithm that beat Cinematch
A week later, on October 15, there were 3 teams already submitting algorithms beating Cinematch, + 1 of these solutions beat
Cinematch by more than 1%, already qualifying for a progress prize.
The contest was hugely popular all over the world. By June 2007, over 20k teams had registered from over 150 countries.
The 2007 progress prize went to a team called BellKor, w/ an 8.43% improvement over Cinematch + the following year, several
teams from across the world joined forces to improve the accuracy even further.
In 2008, the progress prize again went to team BellKor, but this time, the team included members from the team BigChaos in
addition to the original members of BellKor.
This was the last progress prize because another 1% improvement would reach the grand prize goal of 10%.
On 6/26/09, (2.5 yrs) BellKor\'s Pragmatic Chaos, composed of members from 3 different original teams, submitted a 10.05%
improvement over Cinematch, signaling the last call for the contest.
Other teams had 30 days to submit algorithms before the contest closed + these were filled w/intense competition + even more
progress.
When predicting user ratings, what data could be useful? There are 2 main types of data that we could use:
- For every movie in the Netflix database, we have a *ranking from all users who have ranked that movie*
- We know *facts about the movie itself* --> actors, director, genre, year, etc.
Suppose we have the following user ratings for 4 users (Amy, Bob, Carl, Dan) + 4 movies (MiB, Apollo 13, Top Gun, Terminator)
- Ratings are on a 1-5 scale + blank entries mean the user has not rated the movie.
- We could suggest Men in Black to Carl since Amy gave it a rating of 5, + Amy + Carl seem to have similar ratings for the
other movies.
- This technique of using other user\'s ratings to make predictions is called **collaborative filtering**
- Note we\'re not using any info about the movie itself here, just the similarity between users.
Instead, we could use movie info to predict user ratings.
- Amy liked Men in Black, which was directed by Barry Sonnenfeld, is as action, adventure, sci-fi, + comedy, + stars Will
Smith.
- Based on this info, we could make recommendations to Amy w/ the same director, like Get Shorty.
- We can instead recommend Jurassic Park, which is also classified as genres of action, adventure, + sci-fi.
- Or we could recommend to Amy another movie starring Will Smith, like Hitch.
- Note we\'re not using the ratings of other users at all here, just info about the movie + this technique is called
**content filtering**
There are strengths + weaknesses to both types of recommendation systems.
- *Collaborative filtering* can accurately suggest complex items w/out understanding the nature of the items.
- It didn\'t matter at all that our items were movies in the collaborative filtering example.
- We were just comparing user ratings.
- However, this requires a lot of data about a user to make accurate recommendations.
- Also, when there are millions of items, it needs a lot of computing power to compute the user similarities.
- On the other hand, *content filtering* requires very little data to get started.
- But the major weakness of content filtering is that it can be limited in scope.
- You\'re only recommending similar things to what the user has already liked, so the recommendations are often not surprising
or particularly insightful.
Ex: If Amazon a recommendation system for books, + would like to use the *same exact algorithm* for shoes, it would have to
be collaborative filtering, since it can\'t use info about the items.
If Amazon would like to suggest books to users based on previous books they purchased, it would be content filtering since
other users are not involved.
Netflix actually uses a **hybrid recommendation system** that uses both collaborative + content filtering.
- Consider a collaborative filtering approach, where we determine that Amy + Carl have similar preferences.
- We could then do content filtering as well, where we could find that Terminator, which they both liked, is classified in
almost the same set of genres as Starship Troopers.
- So then we could recommend Starship Troopers to both Amy *and* Carl, even though neither of them have seen it before.
- If we were *only* doing *collaborative* filtering, 1 of them would have had to have seen it before.
- And if we were *only* doing *content* filtering, we would only be recommending to 1 user at a time.
So by combining the 2 methods, the algorithm can be much more efficient and accurate.
**MovieLens**, a movie recommendation website run by the GroupLens research lab at the University of
Minnesota, collects user preferences about movies + does collaborative filtering to make recommendations to users based
on the similarities between users.
We\'ll use their movie database to do content filtering using a technique **clustering**
Movies in the MovieLens data set are categorized in 18 different genres, crime, musical, mystery, children\'s, as well as an
unknown category
Each movie may belong to many different genres, so a movie could be classified as drama, adventure, + sci-fi.
The question we want to answer is, **can we systematically find groups of movies with similar sets of genres?** + we\'ll use
a method called **clustering**
Clustering is different from the other analytics methods, as its an **UNSUPERVISED learning method**
-This means we\'re just trying to *segment* data into similar groups, instead of actually trying to *predict* an outcome
The goal of clustering is to put each data point into a group w/ similar values in the data.
A clustering algorithm does NOT *predict* anything, but clustering CAN be used to *improve predictive methods*
- You can cluster data into similar groups + then build a predictive model for each group.
- This can often improve the accuracy of predictive methods, but as a warning, be careful not to over-fit your model to
the training set.
- This works best for large data sets.
There are many different algorithms for clustering + they differ in what makes a cluster + how the clusters are found.
We\'ll cover hierarchical clustering + K-means clustering.
- There are other clustering methods also, but hierarchical + K-means are 2 of the most popular methods.
To cluster data points, we need to compute how *similar* the points are, which is done by computing the distance between
points
The 1st step in clustering is to define the distance between 2 data points.
- The most popular way to compute the distance is w/ the **Euclidean distance**, the standard way to compute distance
- Suppose we have 2 data points, i + j.
- The distance between the two points, d(ij), is equal to the square root of the difference between the 2 points in the
1st component squared PLUS the difference between the 2 points in the 2nd component squared, all the way up to the
difference between the 2 points in the k-th component squared, where k = the number of attributes/independent variables
- d(ij) = Sqrt[ (x(i1) - x(j1))^2 + (x(i2) - x(j2))^2 + ... + (x(ik) - x(jk))^2 ]
In our dataset, we have binary vectors for each movie, classifying that movie into genres.
- Toy Story is categorized as an animation, comedy, + childrens movie, so the data for Toy Story has a 1 in these 3 genres
+ a 0 everywhere else         --> (0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,)
- Batman Forever is categorized as an action, adventure, comedy, + crime movie, so it has a 1 in the spot for these 4
genres + a 0 everywhere else. --> (0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,)
So given these two data observations, let\'s compute the distance between them.
- d = Sqrt( (0-0)^2 + (0-1)^2 + (0-1)^2 + (1-0)^2 .....) = sqrt(4)
In addition to Euclidean distance, there are many other popular distance metrics that could be used.
- Manhattan distance --> distance = sum of the *absolute values* instead of the sum of squares.
- Maximum Coordinate Distance --> only consider the measurement for which the data points deviate the most.
Another important distance that we have to calculate for clustering is the *distance between clusters*, when a cluster = a
group of data points.
We just discussed how to compute the distance between 2 individual points, but how do we compute the distance between groups
of points?
1 way of doing this is by using the **minimum distance**, which defines the distance between clusters as the distance
between the 2 data points in the clusters that are closest together.
- would define the distance between the 2 clusters by computing the Euclidean distance between those 2 points.
The other points in the clusters could be really far away, but it doesn\'t matter if we use minimum distance.
-The only thing we care about is how close together the closest points are.
Alternatively, we could use maximum distance, in which the distance between the 2 clusters is the distance between the 2
points that are the farthest apart.
Here, it doesn\'t matter how close together the other points are, all we care about is how close together the furthest
points are.
The most common distance metric between clusters is called **centroid distance** which defines the distance between clusters
by computing the centroid of the clusters.
The **centroid** = the data point that takes the average of ALL data points in each component, which takes all data points
in each cluster into account + can be thought of as the middle data point.
When computing distances, it\'s highly influenced by the SCALE of the variables.
- Ex: Suppose you\'re computing the distance between 2 data points, where 1 variable is the revenue of a company in
thousands of dollars + another is the age of the company in years.
- The revenue variable would dominate in the distance calculation.
- The differences between the data points for revenue would be in the thousands, whereas the differences between year
data points would probably be less than 10.
- To handle this, it\'s customary to **normalize** the data first.
- We can **normalize** by *subtracting the mean of the data* + *dividing by the standard deviation*
In our movie data set, all of our genre variables are on the same scale, so we don\'t have to worry about normalizing.
- But if we wanted to add a variable, like box office revenue, we would need to normalize so that this variable didn\'t
dominate all others.
Now that we\'ve defined how we\'ll compute the distances, we\'ll talk about a specific clustering algorithm, **hierarchical
clustering**
In hierarchical clustering, the clusters are formed by each data point starting in its *own* cluster.
Ex: Suppose we have 5 data points + each data point is labeled as belonging in its own cluster --> red, blue purple,
green, + yellow cluster
Then hierarchical clustering combines the 2 nearest clusters into 1 cluster + we\'ll use Euclidean + Centroid distances to
decide which 2 clusters are the closest.
In our example, the green + yellow clusters are closest, so we would combine these 2 into 1 cluster, so now the green
cluster has 2 points, + the yellow cluster is gone.
- Now this process repeats --> the new green + the purple clusters are closest now, + we combine them into 1 cluster.
- Now the green cluster has 3 points + the purple cluster is gone.
- Now the 2 nearest clusters are the red + blue clusters, so we combine these 2 clusters into 1 (red) cluster.
- So now we have just 2 clusters, red + green + the final step is to combine these into 1 cluster.
So at the end of hierarchical clustering, all data points are in a single cluster.
The hierarchical cluster process can be displayed through a **dendrogram** w/ the data points listed along the bottom + w/
lines showing how the clusters were combined.
The height of the lines represents how far apart the clusters were when they were combined.
Ex: Points 1 + 4 were pretty close together when they were combined, so their line\'s length is short
But when we combined the two clusters at the end (**down to up**), they were significantly farther apart.
We can use a dendrogram to decide how many clusters we want for our final clustering model.
- The easiest way to pick the # of clusters you want is to draw a horizontal line across the dendrogram.
- The number of vertical lines that line crosses is the # of clusters there will be.
- Ex of dendrogram w/ 10 points + a line crossing 2 vertical lines --> we will have 2 clusters, 1 w/ points 5, 2, + 7,
+ 1 w/ the remaining points.
- The farthest this ad-hoc horizontal line can move up + down in the dendrogram w/out hitting one of the horizontal lines
of the dendrogram, the better that choice of the # of clusters is.
- If we instead selected a line that hits 3 clusters, the line can\'t move as far up + down w/out hitting horizontal lines
in the dendrogram.
- This probably means that the two cluster choice is better.
- But when picking # of clusters, also consider how many clusters make sense for the particular application you\'re
working w/.
After selecting the number of clusters you want, analyze your clusters to see if they\'re meaningful by looking at basic
statistics in each cluster, (mean, maximum, minimum each cluster + each variable)
Can also check to see if the clusters have a feature in common that was NOT used in the clustering, like an outcome variable
- This often indicates that your clusters might help improve a predictive model.'
setwd("C:/Users/snewns/Dropbox/NewLearn/Edx/AnalyticsEdge/Data")
movies = read.table("movies.txt", header=FALSE, sep="|",quote="\"")
str(movies)
'Data is not a CSV, but a text file, where entries are separated by a vertical bar, w/ 1,682 observations of 24 variables
- Since our variables didn\'t have names, header = FALSE, R just labeled them w/ V1, V2, V3, etc.
- But from the Movie Lens documentation, we know what these are.'
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation",
"Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance",
"SciFi", "Thriller", "War", "Western")
str(movies)
'We won\'t be using the ID, release date, video release date, or IMDB variables, so remove them.'
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL
'There are a few duplicate entries in our data set, so remove them w/ unique()'
movies = unique(movies)
str(movies)
'Now, we have 1,664 observations, a few less than before + 20 variables = title movie, unknown genre + 18 other genres'
table(movies$Comedy)
table(movies$Western)
table(movies$Romance,movies$Drama)
'2 steps to hierarchical clustering.
- 1 --> Compute the distances between all data points
- 2 --> Need to cluster the points.
To compute the distances, use dist(), + we only want to cluster movies on genre, so cluster on columns 2-20'
distances <- dist(movies[2:20], method = "euclidean")
clusterMovies <- hclust(distances, method = "ward")
'ward arg cares about the distance between clusters using centroid distance, + also the variance in each of the clusters.'
#plot the dendrogram of our clustering algorithm
plot(clusterMovies)
'All data points are along the bottom + w/ over 1k data points, it\'s impossible to read.
So looking at this dendrogram, how many clusters
would you pick?
It looks like maybe 3 or 4 clusters would be a good choice according to the dendrogram, (space moving up + down), but
let\'s keep our application in mind, too.
We probably want more than 2, 3, or even 4 clusters of movies to make recommendations to users, + it looks like there\'s a
nice spot down where there\'s 10 clusters, which is probably better for our application.
We could select even more clusters if we want to have *very specific* genre groups.
- If you want a lot of clusters it\'s hard to pick the right number from the dendrogram.
- You need to use your understanding of the problem to pick the number of clusters.
We can label each of the data points according to what cluster it belongs to using cutree()'
clusterGroups <- cutree(clusterMovies,k=10) #for 10 clusters
#Find % of movies in each genre + cluster --> see what % of movies in each cluster are Action movies
tapply(movies$Action,clusterGroups,mean)
'This divides our data points into 10 clusters + then computes the average value of Action for each cluster.
- Remember Action is a binary variable w/ value 0 or 1, so by computing the average of this variable, we\'re computing
the % of movies in that cluster that belong in that genre.
In Cluster 2, ~78% of movies are Action, while no movies in Cluster 4 are.'
#Try w/ romance
tapply(movies$Romance,clusterGroups,mean)
#All movies in Clusters 6 + 7 are Romance, while none in Clusters 4, 5, 8, 9, or 10 are
'We can repeat this for each genre.
- If you do you can create a large table to better analyze the clusters + have it saved to a spreadsheet.
- Each column is a cluster + in each row, the genre, and the cell values = %
- Can highlight cells w/ a higher than average value, like Cluster 2 + Action movies
- Cluster 1 = a little bit of everything, so we\'re calling it the miscellaneous cluster.
- Cluster 2 = a lot of the action, adventure, + sci-fi movies.
- Cluster 3 = crime, mystery, thriller movies.
- Cluster 4 = exclusively has drama movies.
- Cluster 5 = exclusively has comedies.
- Cluster 6 = a lot of the romance movies.
- Cluster 7 = comedies + romance movies (RomComs)
- Cluster 8 = the documentaries.
- Cluster 9 = comedies + dramas (Dramadies)
- Cluster 10 = the horror flicks.
Knowing common movie genres, these clusters seem to make a lot of sense.
Let\'s see how these clusters could be used in a recommendation system.
- Remember Amy liked Men in Black, so let\'s figure out what cluster Men in Black is in.
Use subset() to take a subset of movies + only look at the movies where Title="Men in Black (1997)".'
subset(movies, Title == "Men in Black (1997)")
#So it's record 257 and is Action, Adv, Comedy, and SciFi --> Which cluster is it in?
clusterGroups[257]
'So it\'s in Cluster 2 + that make sense since we just saw that cluster 2 is the action, adventure, sci-fi cluster.
Create a new data set w/ just movies from cluster 2'
cluster2 <- subset(movies, clusterGroups == 2)
cluster2$Title[1:10]
'So this looks like good movies to recommend to Amy, according to our clustering algorithm, would be movies like Apollo 13
+ Jurassic Park'
clusterGroups2 <- cutree(clusterMovies,k=2) #for 2 clusters
#table(clusterGroups)
tapply(movies$Action,clusterGroups,mean)
table(clusterGroups2)
tapply(movies$Action,clusterGroups,mean)
tapply(movies$Action,clusterGroups2,mean)
tapply(movies$Adventure,clusterGroups2,mean)]
tapply(movies$Adventure,clusterGroups2,mean)
tapply(movies$Animation,clusterGroups2,mean)
tapply(movies$Childrens,clusterGroups2,mean)
tapply(movies$Comedy,clusterGroups2,mean)
tapply(movies$SciFi,clusterGroups2,mean)
tapply(movies$Thriller,clusterGroups2,mean)
tapply(movies$Horror,clusterGroups2,mean)
tapply(movies$Fantasy,clusterGroups2,mean)
tapply(movies$Drama,clusterGroups2,mean)
colMeans(subset(movies[2:20], clusterGroups == 1))
spl = split(movies[2:20], clusterGroups)
lapply(spl, colMeans)
spl2 = split(movies[2:20], clusterGroups2)
lapply(spl2, colMeans)
setwd("C:/Users/snewns/Dropbox/RunningAnalysis/Data")
strava <- read.csv("strava.csv")
dim(strava)
head(strava)
str(newStrava$Date)
summary(newGarmin)
setwd("C:/Users/snewns/Dropbox/RunningAnalysis/Data")
#setwd("C:/Users/Nimz/Dropbox/RunningAnalysis/Data")
strava <- read.csv("strava.csv")
str(strava)
head(strava)
#remove unneccessary cols
keepCols <- NA
keepCols <- c("Activity.Id", "When", "Type", "Gear", "Name", "Dist.mi", "Elv.ft", "Elapsed.Time", "Moving.Time",
"Speed.mph", "Pace..mi", "Max.Pace..mi", "Cad", "Heart", "Max.Heart", "Elev.Dist.ft.mi",
"Elev.Time.ft.h", "Cal", "Segs", "PRs", "Kudos")
newStrava <- strava[keepCols]
str(newStrava)
head(newStrava)
tail(newStrava)
##PRIOR DATA CLEANING --> REMOVE 2 ROWS AT TOP OF GARMIN FILES
garmin1 <- read.csv("garmin1.csv")
#check for interesting cols from garmin to keep
str(garmin1)
names(garmin1)
#Look at them more closely
head(garmin1)
tail(garmin1)
#get most recent data
garmin6 <- read.csv("garmin6.csv")
head(garmin6)
tail(garmin6)
#load in rest of garmin files and combine into 1 DF
garmin2 <- read.csv("garmin2.csv")
garmin3 <- read.csv("garmin3.csv")
garmin4 <- read.csv("garmin4.csv")
garmin5 <- read.csv("garmin5.csv")
head(garmin5)
tail(garmin4)
garminFull <- Reduce(function(...) merge(..., all=TRUE), list(garmin1, garmin2, garmin3, garmin4,
garmin5, garmin6))
summary(garminFull)
head(garminFull)
garminFull <- garminFull[-c(1,2),]
head(garminFull)
str(garminFull)
summary(garminFull)
#multiple runs on August 12? Maybe a bike ride?
#check type in Strava
#install.packages("stringr")
library(stringr)
newStrava$Date <- str_split_fixed(newStrava$When, " ", 2)[,1]
newStrava$StartTime <- str_split_fixed(newStrava$When, " ", 2)[,2]
which(newStrava$Date == '8/12/2016')
newStrava[95:96,]
#looks like I just ran twice that day in Sea Isle. Back to the Garmin data cleaning
head(garminFull)
#install.packages("tidyr")
library(tidyr)
# specify the new column names:
vars <- c("Date", "StartTime")
vars2 <- c("DOW", "Date")
# then separate the "Details" column according to regex and drop extra columns:
garminFull <- separate(garminFull, Start, into = vars, sep = "(?<=6 )", extra = "merge", remove = TRUE)
garminFull <- separate(garminFull, Date, into = vars2, sep = ", ", extra = "merge", remove = TRUE)
garminFull[order(garminFull$Date),]
garminFull$monthNum <- ifelse(grepl("Jan",garminFull$Date),1,
ifelse(grepl("Feb",garminFull$Date),2,
ifelse(grepl("Mar",garminFull$Date),3,
ifelse(grepl("Apr",garminFull$Date),4,
ifelse(grepl("May",garminFull$Date),5,
ifelse(grepl("Jun",garminFull$Date),6,
ifelse(grepl("Jul",garminFull$Date),7,
ifelse(grepl("Aug",garminFull$Date),8,
ifelse(grepl("Sep",garminFull$Date),9,
ifelse(grepl("Oct",garminFull$Date),10,
ifelse(grepl("Nov",garminFull$Date),11,
ifelse(grepl("Dec",garminFull$Date),12,NA))))))))))))
head(garminFull)
vars3 <- c("Month", "Date")
garminFull <- separate(garminFull, Date, into = vars3, sep = " ", extra = "merge", remove = TRUE)
head(garminFull)
vars4 <- c("Day", "Year")
garminFull <- separate(garminFull, Date, into = vars4, sep = ", ", extra = "merge", remove = TRUE)
head(garminFull)
garminFull$Year <- trimws(garminFull$Year)
garminFull$Date <- format(as.Date(with(garminFull, paste(Year, monthNum, Day,sep="-")), "%Y-%m-%d"), "%m/%d/%Y")
str(garminFull$Date)
keepColsGarmin  <- NA
keepColsGarmin <- c("DOW", "Month", "StartTime", "Time", "Distance", "Elevation.Gain", "Avg.Speed.Avg.Pace.", "Avg.HR",
"Max.HR", "Calories", "Date", "monthNum")
newGarmin <- garminFull[keepColsGarmin]
head(newGarmin)
newGarmin$Date <- as.Date(newGarmin$Date,"%m/%d/%Y")
newStrava$Date <- as.Date(newStrava$Date,"%m/%d/%Y")
str(newGarmin$Date)
str(newStrava$Date)
newGarmin <- newGarmin[order(newGarmin$Date, decreasing = FALSE),]
newStrava <- newStrava[order(newStrava$Date, decreasing = FALSE),]
head(newGarmin)
head(newStrava)
#3 extra Garmin runs in Jan 2016, remove
summary(newGarmin$Date)
newGarmin <- newGarmin[!newGarmin$Date == c("2016-01-20","2016-01-30","2016-01-31")]
#newGarmin <- newGarmin[!newGarmin$Date == "2016-01-30",]
#newGarmin <- newGarmin[!newGarmin$Date == "2016-01-31",]
newGarmin <- newGarmin[!newGarmin$Date < "2016-07-18",]
newStrava <- newStrava[!newStrava$Date < "2016-07-18",]
nrow(newGarmin) #3 extra
nrow(newStrava)
tail(newGarmin)
tail(newStrava)
newGarmin[newGarmin$Distance=="0.12"]
newGarmin[newGarmin$Distance=="0.16"]
newGarmin[newGarmin$Distance=="1.42"]
setwd("C:/Users/snewns/Dropbox/RunningAnalysis/Data")
#setwd("C:/Users/Nimz/Dropbox/RunningAnalysis/Data")
strava <- read.csv("strava.csv")
str(strava)
head(strava)
#remove unneccessary cols
keepCols <- NA
keepCols <- c("Activity.Id", "When", "Type", "Gear", "Name", "Dist.mi", "Elv.ft", "Elapsed.Time", "Moving.Time",
"Speed.mph", "Pace..mi", "Max.Pace..mi", "Cad", "Heart", "Max.Heart", "Elev.Dist.ft.mi",
"Elev.Time.ft.h", "Cal", "Segs", "PRs", "Kudos")
newStrava <- strava[keepCols]
str(newStrava)
head(newStrava)
tail(newStrava)
##PRIOR DATA CLEANING --> REMOVE 2 ROWS AT TOP OF GARMIN FILES
garmin1 <- read.csv("garmin1.csv")
#check for interesting cols from garmin to keep
str(garmin1)
names(garmin1)
#Look at them more closely
head(garmin1)
tail(garmin1)
#get most recent data
garmin6 <- read.csv("garmin6.csv")
head(garmin6)
tail(garmin6)
#load in rest of garmin files and combine into 1 DF
garmin2 <- read.csv("garmin2.csv")
garmin3 <- read.csv("garmin3.csv")
garmin4 <- read.csv("garmin4.csv")
garmin5 <- read.csv("garmin5.csv")
head(garmin5)
tail(garmin4)
garminFull <- Reduce(function(...) merge(..., all=TRUE), list(garmin1, garmin2, garmin3, garmin4,
garmin5, garmin6))
summary(garminFull)
head(garminFull)
garminFull <- garminFull[-c(1,2),]
head(garminFull)
str(garminFull)
summary(garminFull)
#multiple runs on August 12? Maybe a bike ride?
#check type in Strava
#install.packages("stringr")
library(stringr)
newStrava$Date <- str_split_fixed(newStrava$When, " ", 2)[,1]
newStrava$StartTime <- str_split_fixed(newStrava$When, " ", 2)[,2]
which(newStrava$Date == '8/12/2016')
newStrava[95:96,]
#looks like I just ran twice that day in Sea Isle. Back to the Garmin data cleaning
head(garminFull)
#install.packages("tidyr")
library(tidyr)
# specify the new column names:
vars <- c("Date", "StartTime")
vars2 <- c("DOW", "Date")
# then separate the "Details" column according to regex and drop extra columns:
garminFull <- separate(garminFull, Start, into = vars, sep = "(?<=6 )", extra = "merge", remove = TRUE)
garminFull <- separate(garminFull, Date, into = vars2, sep = ", ", extra = "merge", remove = TRUE)
garminFull[order(garminFull$Date),]
garminFull$monthNum <- ifelse(grepl("Jan",garminFull$Date),1,
ifelse(grepl("Feb",garminFull$Date),2,
ifelse(grepl("Mar",garminFull$Date),3,
ifelse(grepl("Apr",garminFull$Date),4,
ifelse(grepl("May",garminFull$Date),5,
ifelse(grepl("Jun",garminFull$Date),6,
ifelse(grepl("Jul",garminFull$Date),7,
ifelse(grepl("Aug",garminFull$Date),8,
ifelse(grepl("Sep",garminFull$Date),9,
ifelse(grepl("Oct",garminFull$Date),10,
ifelse(grepl("Nov",garminFull$Date),11,
ifelse(grepl("Dec",garminFull$Date),12,NA))))))))))))
head(garminFull)
vars3 <- c("Month", "Date")
garminFull <- separate(garminFull, Date, into = vars3, sep = " ", extra = "merge", remove = TRUE)
head(garminFull)
vars4 <- c("Day", "Year")
garminFull <- separate(garminFull, Date, into = vars4, sep = ", ", extra = "merge", remove = TRUE)
head(garminFull)
garminFull$Year <- trimws(garminFull$Year)
garminFull$Date <- format(as.Date(with(garminFull, paste(Year, monthNum, Day,sep="-")), "%Y-%m-%d"), "%m/%d/%Y")
str(garminFull$Date)
keepColsGarmin  <- NA
keepColsGarmin <- c("DOW", "Month", "StartTime", "Time", "Distance", "Elevation.Gain", "Avg.Speed.Avg.Pace.", "Avg.HR",
"Max.HR", "Calories", "Date", "monthNum")
newGarmin <- garminFull[keepColsGarmin]
head(newGarmin)
newGarmin$Date <- as.Date(newGarmin$Date,"%m/%d/%Y")
newStrava$Date <- as.Date(newStrava$Date,"%m/%d/%Y")
str(newGarmin$Date)
str(newStrava$Date)
newGarmin <- newGarmin[order(newGarmin$Date, decreasing = FALSE),]
newStrava <- newStrava[order(newStrava$Date, decreasing = FALSE),]
head(newGarmin)
head(newStrava)
#3 extra Garmin runs in Jan 2016, remove
summary(newGarmin$Date)
newGarmin <- newGarmin[!newGarmin$Date == c("2016-01-20","2016-01-30","2016-01-31")]
#newGarmin <- newGarmin[!newGarmin$Date == "2016-01-30",]
#newGarmin <- newGarmin[!newGarmin$Date == "2016-01-31",]
newGarmin <- newGarmin[!newGarmin$Date < "2016-07-18",]
newStrava <- newStrava[!newStrava$Date < "2016-07-18",]
nrow(newGarmin) #3 extra
nrow(newStrava)
tail(newGarmin)
tail(newStrava)
newGarmin[newGarmin$Distance==0.12]
newGarmin[newGarmin$Distance==0.16]
newGarmin[newGarmin$Distance==1.42]
newGarmin$Date == "2016-10-23"
